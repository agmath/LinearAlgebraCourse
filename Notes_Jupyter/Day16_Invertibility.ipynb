{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmuc/Pu99jB1MReCZo4CHm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JnFbhs9cQIr0","executionInfo":{"status":"ok","timestamp":1749475771313,"user_tz":240,"elapsed":10495,"user":{"displayName":"Adam Gilbert","userId":"02074648007699947871"}}},"outputs":[],"source":["import numpy as np\n","import sympy as sp\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## Warm-Up Problems\n","\n","Consider the following problems as we wait for class to begin."],"metadata":{"id":"Rl1W5KyLQQFy"}},{"cell_type":"markdown","source":["**Warm-Up Problem 1:** Let $A = \\left[\\begin{array}{rr} 3 & 1 & -2\\\\ 1 & 0 & 5\\end{array}\\right]$, $B = \\left[\\begin{array}{rrr} -1 & 1 & 0\\\\ 3 & 4 & -2\\\\ 2 & 0 & 1\\end{array}\\right]$, and $C = \\left[\\begin{array}{rr} 2 & 1\\\\ -3 & 0\\\\ 0 & 1\\end{array}\\right]$.\n","\n","+ Which of the products $AB$, $BA$, $AC$, $CA$, $BC$, and $CB$ are possible to compute? Why/Why not?\n","+ Compute the product $AC$.\n","+ Compute the scalar-matrix product $-5B^T$.\n","+ Compute $AC - 5B^T$ if it is possible to do so."],"metadata":{"id":"tUSjZSinQa4C"}},{"cell_type":"markdown","source":["# Day 13: The Inverse of a Matrix\n","\n","We've been focused for some time now on solving the matrix equation $A\\vec{x} = \\vec{b}$ (and its equivalent variations). None of the approaches we've utilized so far are as simple as the approach you used to solve equations like $2x = 4$ -- just *divide both sides by $2$*. There's a reason for this, and it was hinted at in the last notebook. Division by a matrix is not a well-defined operation and we can't simply \"divide both sides of an equation by a coefficient matrix\". Indeed, we saw in the last notebook that $AB = AC$ does not necessarily mean $B = C$. Matrices have some properties that are not familiar to us:\n","\n","+ Matrix multiplication is *not commutative*. That is, in general, $AB \\neq BA$.\n","+ Matrix multiplication does not have a cancellation rule. That is, in general, $AB = AC$ does not mean that $B = C$.\n","+ There is no \"matrix division\" operation."],"metadata":{"id":"ul0xpRnaQi37"}},{"cell_type":"markdown","source":["### Motivation\n","\n","Consider the following four equations.\n","\n","$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$\n","\n","You've seen and solved equations like this hundreds of times. When you solved them (those that are solvable, anyway), you likely made use of a *division* operation. Let's revisit these equations and their solutions, but since we don't have a *division* operation with matrices, let's try to solve them without using *division*!\n","\n","Let's walk through solving the first equation, $5x = 35$ slowly by considering a few observations.\n","\n","> **Example (i):** $5x = 35$\n",">\n","> + The left-hand side of the equation contains the expression \"*$5$ times $x$\"*\n","> + We'd like to manipulate the equation so that the left-hand side contains only the expression \"*$x$*\".\n","> + Perhaps we can settle for \"*$1x$*\", since $1$ is the *multiplicative identity* -- that is, $1x = x$ for all possible values of $x$.\n","> + We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?\n","> + Let's multiply both sides of our equation by $\\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain\n",">\n","> \\begin{align} 5x &= 35\\\\\n","\\implies \\frac{1}{5}\\left(5x\\right) &= \\implies \\frac{1}{5}\\left(35\\right)\\\\\n","\\implies \\left(\\frac{1}{5}\\cdot 5\\right)x &= 7\\\\\n","\\implies 1x &= 7\\\\\n","\\implies x &= 7\n","\\end{align}\n",">\n",">  + ***Note*:** The scalar $\\frac{1}{5}$ is the *multiplicative inverse* of the scalar $5$ since $\\frac{1}{5}\\cdot 5 = 5\\cdot \\frac{1}{5} = 1$, where $1$ is the *multiplicative identity*.\n","\n","**Summary:** We can solve the *scalar equation* $ax = b$ by multiplying both sides of the equation by the *multiplicative inverse* of $a$, provided that it exists!\n","\n","Let's solve the other equations:\n","\n","> **Equation (ii):** $2x = 0$\n",">\n",">\\begin{align} 2x&= 0\\\\\n","\\implies \\frac{1}{2}\\left(2x\\right) &= \\frac{1}{2}\\left(0\\right)\\\\\n","\\implies \\left(\\frac{1}{2}\\cdot 2\\right)x &= 0\\\\\n","\\implies 1x &= 0\\\\\n","\\implies x &= 0\n","\\end{align}\n","\n","> **Equation (iii):** $0x = 2$...We run into a little bit of a problem here. There is no multiplicative inverse of $0$. However, we do know that the product $0x$ for any value of $x$ will be $0$. This, this equation has no solutions -- it is *inconsistent*.\n","\n","> **Equation (iv):** $0x = 0$...Again, we run into a problem. There is no multiplicative inverse of $0$, so we cannot find a solution. Inspection however, shows us that any value of $x$ will satisfy this equation -- there are infinitely many solutions!\n","\n","**Summary:** The scalar equation $ax = b$ has a *unique solution* if the multiplicative inverse of $a$ (that is, $\\frac{1}{a}$) exists. Otherwise, the equation either has no solutions, or infinitely many solutions."],"metadata":{"id":"cvqpgE6Jul-5"}},{"cell_type":"markdown","source":["### Invertible Matrices\n","\n","Recall that *linear transformations* (a special class of function) correspond to matrix multiplication. You might remember the following two properties that some functions satisfy.\n","\n","+ **One-to-One:** A function $f$ is said to be *one-to-one* if $f(x_1) = f(x_2)$ implies that $x_1 = x_2$. That is, for each output there is exactly one input being mapped to it.\n","\n","+ **Onto:** A function $f$ is said to be *onto* if for every $b$ in the codomain of $f$, there is some $x$ in the domain such that $f\\left(x\\right) = b$.\n","\n","These properties are important because if a function is <u>both</u> *one-to-one* and *onto*, then that function is ***invertible***.\n","\n","We can argue that a linear transformation $T\\left(\\vec{x}\\right) = A\\vec{x}$ is one-to-one if the matrix $A$ has a pivot in every column (no free variables $\\implies$ unique solutions), and that the transformation is *onto* if the matrix $A$ has a pivot in every row (it is impossible then for $\\begin{bmatrix} A & | & \\vec{b}\\end{bmatrix}$ to have a pivot in its rightmost column, so $A\\vec{x} = \\vec{b}$ is consistent for all choices of $\\vec{b}$).\n","\n","**Summary/Theorem:** The above properties and discussion argue that the *linear transformation* $T\\left(\\vec{x}\\right) = A\\vec{x}$ is *invertible* if and only if the matrix $A$ has a pivot in every row and every column. This means that matrix multiplication via the matrix $A$ is invertible in these scenarios.\n","\n","> **Note:** Only square matrices can have a pivot in every row *and* every column. Thus only square matrices are invertible, but not every square matrix is invertible.\n","\n","**Definition (Matrix Inverse):** Given an $n\\times n$ matrix $A$, a matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I_n$ is said to be the inverse of $A$.\n","\n","**Definition (Singular and Non-Singular Matrices):** A matrix whose inverse exists is said to be a *non-singular* matrix, whereas a matrix whose inverse does not exist is said to to be *singular*."],"metadata":{"id":"QZabKNMEUSHr"}},{"cell_type":"markdown","source":["> Note that the following is a *shortcut* for computing the inverse of a $2\\times 2$ matrix. A general strategy is introduced before the *Try It!* section at the end of this notebook.\n","\n","**Computing the Inverse of a $2\\times 2$ Matrix:** Let $A = \\left[\\begin{array}{rr} a & b\\\\ c & d\\end{array}\\right]$ be a $2\\times 2$ matrix and assume $ad - bc \\neq 0$, then $A$ is invertible and $A^{-1} = \\frac{1}{ad - bc}\\left[\\begin{array}{rr} d & -b\\\\ -c & a\\end{array}\\right]$.\n","\n","**Note:** If $A = \\left[\\begin{array}{rr} a & b\\\\ c & d\\end{array}\\right]$ and $ad - bc = 0$, then $A$ is *singular* and $A^{-1}$ does not exist.\n","\n","**Note:** The quantity $ad - bc$ is called the *determinant* of the $2\\times 2$ matrix $A = \\left[\\begin{array}{rr} a &b\\\\ c &d\\end{array}\\right]$. We write this as $\\det\\left(A\\right) = ad - bc$. We'll discuss determinants in greater detail later in our course."],"metadata":{"id":"yX0BG6S8Wg30"}},{"cell_type":"markdown","source":["**Theorem (Invertible Matrices and Solving $A\\vec{x} = \\vec{b}$):** If the matrix $A$ is invertible, then the matrix equation $A\\vec{x} = \\vec{b}$ has a unique solution for all $\\vec{b}$. That unique solution is $\\vec{x} = A^{-1}\\vec{b}$.\n","\n","**Theorem (Properties of Invertible Matrices):** The following properties about matrices and their inverses are true.\n","\n","+ If $A$ is an invertible matrix, then $A^{-1}$ is also invertible and $\\left(A^{-1}\\right)^{-1} = A$.\n","+ If $A$ and $B$ are invertible $n\\times n$ matrices, then $AB$ is also an invertible $n\\times n$ matrix. The inverse of $AB$ is $\\left(AB\\right)^{-1} = B^{-1}A^{-1}$.\n","+ If $A$ is an invertible matrix then so is $A^T$, and the inverse of $A^T$ is $\\left(A^T\\right)^{-1} = \\left(A^{-1}\\right)^T$.\n"],"metadata":{"id":"8u4bIfldZ41d"}},{"cell_type":"markdown","source":["**Strategy (Constructing the Inverse of a Matrix):** The theorem above gives us a strategy for constructing the inverse of a matrix as long as the inverse exists.\n","\n","1. Start with an augmented matrix of the form $\\left[A\\mid I_n\\right]$ -- this augmented matrix has the full identity matrix on the right side of that dividing line rather than just a vector.\n","2. Use our old row-reducing techniques to transform the left hand side of the augmented matrix into the identity matrix. The resuting matrix on the right is $A^{-1}$."],"metadata":{"id":"mUh0szQZT99e"}},{"cell_type":"markdown","source":["**Completed Example 1:** Compute the inverse of the matrix $A = \\left[\\begin{array}{rrr} 1 & 3 & -2\\\\ 0 & 5 & 4\\\\ 2 & 1 & 8\\end{array}\\right]$ if it exists.\n","\n","> *Solution.* We'll augment the matrix by the $3\\times 3$ identity matrix and then use row-reduction to obtain the inverse.\n",">\n",">\\begin{align*} \\left[\\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\\\ 0 & 5 & 4 & 0 & 1 & 0\\\\ 2 & 1 & 8 & 0 & 0 & 1\\end{array}\\right] &\\substack{R_3\\leftarrow R_3 + (-2R_1)\\\\ \\longrightarrow} \\left[\\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\\\ 0 & 5 & 4 & 0 & 1 & 0\\\\ 0 & -5 & 12 & -2 & 0 & 1\\end{array}\\right]\\\\\n","&\\substack{R_3\\leftarrow R_3 + R_2\\\\ \\longrightarrow} \\left[\\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\\\ 0 & 5 & 4 & 0 & 1 & 0\\\\ 0 & 0 & 16 & -2 & 1 & 1\\end{array}\\right]\\\\\n","&\\substack{R_2\\leftarrow (1/5)R_2\\\\ R_3\\leftarrow (1/16)R_3\\\\ \\longrightarrow} \\left[\\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\\\ 0 & 1 & 4/5 & 0 & 1/5 & 0\\\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\\end{array}\\right]\\\\\n","&\\substack{R_2\\leftarrow R_2 + (-4/5R_3)\\\\ \\longrightarrow} \\left[\\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\\end{array}\\right]\\\\\n","&\\substack{R_1\\leftarrow R_1 + 2R_3\\\\ \\longrightarrow} \\left[\\begin{array}{rrr|rrr} 1 & 3 & 0 & 3/4 & 1/8 & 1/8\\\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\\end{array}\\right]\\\\\n","&\\substack{R_1\\leftarrow R_1 + (-3R_2)\\\\ \\longrightarrow} \\left[\\begin{array}{rrr|rrr} 1 & 0 & 0 & 9/20 & -13/40 & 11/40\\\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\\end{array}\\right]\\\\\n","\\end{align*}\n",">\n","> So $\\boxed{~\\displaystyle{A^{-1} = \\left[\\begin{array}{rrr} 9/20 & -13/40 & 11/40\\\\ 1/10 & 3/20 & -1/20\\\\ -1/8 & 1/16 & 1/16\\end{array}\\right]}~}$. $_\\blacktriangledown$"],"metadata":{"id":"bci6aiEMM9EA"}},{"cell_type":"markdown","source":["## Try It!\n","\n","The following examples will be used as classwork."],"metadata":{"id":"Ekg80SJBgkyy"}},{"cell_type":"markdown","source":["**Try It! 1:** Find the inverse of the matrix $\\left[\\begin{array}{rr} 2 & 3\\\\ -1 & 5\\end{array}\\right]$.\n","\n","> *Solution.*"],"metadata":{"id":"FkMPNH46gpKK"}},{"cell_type":"markdown","source":["**Try It! 2:** Find the inverse of the matrix $\\left[\\begin{array}{rrr} 1 & 0 & 1\\\\ -2 & 2 & 4\\\\ 0 & 6 & -4\\end{array}\\right]$.\n","\n","> *Solution.*"],"metadata":{"id":"MXS3ookTGSyU"}},{"cell_type":"markdown","source":["**Try It! 3:** Solve the system $\\left\\{\\begin{array}{rlr} 2x_1 + 3x_2 & = & 8\\\\ -x_1 + 5x_2 & = & 9\\end{array}\\right.$ by constructing the corresponding matrix equation and left-multiplying both sides by the inverse of the coefficient matrix.\n","\n","> *Solution.*"],"metadata":{"id":"Cjv17l7GGnzT"}},{"cell_type":"markdown","source":["**Try It! 4a:** Consider the matrix $A = \\left[\\begin{array}{rr} 5 & -4\\\\ 1 & 2\\end{array}\\right]$ and the vectors $\\vec{b_1} = \\left[\\begin{array}{r} -2\\\\ 7\\end{array}\\right]$, $\\vec{b_2} = \\left[\\begin{array}{r} -15\\\\ -9\\end{array}\\right]$, $\\vec{b_3} = \\left[\\begin{array}{r} -6\\\\ 10\\end{array}\\right]$. Find $A^{-1}$ and use it to solve each of the matrix equations $A\\vec{x} = \\vec{b_i}$ for $i = 1, 2, 3$.\n","\n","> *Solution.*"],"metadata":{"id":"cEabNlVAHMVg"}},{"cell_type":"markdown","source":["**Try It! 4b:** Note that the matrix $A^{-1}$ does not depend on the form of any of the vectors $\\vec{b_i}$ in the previous example. This means that the system $A\\vec{x} = \\vec{b}$ is solved by the same set of row operations, regardless of the constant vector $\\vec{b}$. Solve each of the matrix equations $A\\vec{x} = \\vec{b_i}$ simultaneously by row reducing the augmented coefficient matrix $\\left[\\begin{array}{r|rrr} A & \\vec{b_1} & \\vec{b_2} & \\vec{b_3}\\end{array}\\right]$.\n","\n","> *Solution.*"],"metadata":{"id":"jaJTORXxH-ca"}},{"cell_type":"code","source":[],"metadata":{"id":"e2pucsxlwGft"},"execution_count":null,"outputs":[]}]}