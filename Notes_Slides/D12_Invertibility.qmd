---
title: "MAT 350: Invertibility"
author: Dr. Gilbert
format: 
  revealjs:
    smaller: true
date: today
date-format: long
theme: serif
incremental: true
---

```{r global-options, include=FALSE}
library(tidyverse)
library(reticulate)

theme_set(theme_bw(base_size = 20))
```

```{python}
import numpy as np
import sympy as sp
import matplotlib.pyplot as plt

from ipywidgets import interact, FloatText
import matplotlib.patches as patches
```

```{css}
code.sourceCode {
  font-size: 1.3em;
  /* or try font-size: xx-large; */
}

a {
  color: purple;
}

a:link {
  color: purple;
}

a:visited {
  color: purple;
}
```

## Warm-Up Problems

:::{.nonincremental}

Complete the following warm-up problems to re-familiarize yourself with concepts we'll be leveraging today.

1. Let $A = \left[\begin{array}{rr} 3 & 1 & -2\\ 1 & 0 & 5\end{array}\right]$, $B = \left[\begin{array}{rrr} -1 & 1 & 0\\ 3 & 4 & -2\\ 2 & 0 & 1\end{array}\right]$, and $C = \left[\begin{array}{rr} 2 & 1\\ -3 & 0\\ 0 & 1\end{array}\right]$.

    + Which of the products $AB$, $BA$, $AC$, $CA$, $BC$, and $CB$ are possible to compute? Why/Why not?
    + Compute the product $AC$.
    + Compute the scalar-matrix product $-5B^T$.
    + Compute $AC - 5B^T$ if it is possible to do so.

2.  Let $A = \left[\begin{array}{rr} 3 & -6\\ -1 & 2\end{array}\right]$, $B = \left[\begin{array}{rr} -1 & 1\\ 3 & 4\end{array}\right]$, and $C = \left[\begin{array}{rr} -3 & -5\\ 2 & 1\end{array}\right]$. Verify that $AB = AC$ but $B \neq C$.

:::

## Reminders and Today's Goal

. . . 

::::{.columns}

:::{.column width=50%}

Our discussion of linear algebra, so far, has taken us through the following contexts

+ Solving linear systems of equations
+ Solving vector equations
+ Solving matrix equations

:::

:::{.column width=50%}

:::

::::

## Reminders and Today's Goal

::::{.columns}

:::{.column width=50%}

Our discussion of linear algebra, so far, has taken us through the following contexts

:::{.nonincremental}

+ Solving linear systems of equations
+ Solving vector equations
+ Solving matrix equations

:::

We saw that these types of equations are all analogous to one another, and that they can be solved in the same way.

:::

:::{.column width=50%}

:::

::::

## Reminders and Today's Goal

::::{.columns}

:::{.column width=50%}

Our discussion of linear algebra, so far, has taken us through the following contexts

:::{.nonincremental}

+ Solving linear systems of equations
+ Solving vector equations
+ Solving matrix equations

:::

We saw that these types of equations are all analogous to one another, and that they can be solved in the same way.

:::

:::{.column width=50%}

**Solution Strategy:** 

1. Construct an *augmented coefficient matrix*.
2. Use our three *permissible row operations* to reduce the matrix to either *row echelon form* or *reduced row echelon form*.
3. Identify the locations of the *pivots*.
4. Use the number and location of the *pivots* in order to determine *basic variables* and any *free variables* which tells us about the *geometry of the solution space*.
5. Describe the *solution space* using *parametric vector form*.

:::

::::

## Reminders and Today's Goal

::::{.columns}

:::{.column width=50%}

Our discussion of linear algebra, so far, has taken us through the following contexts

:::{.nonincremental}

+ Solving linear systems of equations
+ Solving vector equations
+ Solving matrix equations

:::

We saw that these types of equations are all analogous to one another, and that they can be solved in the same way.

This is a solid strategy, but it is tedious and isn't reusable. Solving an equation for $\vec{b_1}$ and $\vec{b_2}$ has required us to perform the full row-reduction twice($^*$).

:::

:::{.column width=50%}

**Solution Strategy:** 

:::{.nonincremental}

1. Construct an *augmented coefficient matrix*.
2. Use our three *permissible row operations* to reduce the matrix to either *row echelon form* or *reduced row echelon form*.
3. Identify the locations of the *pivots*.
4. Use the number and location of the *pivots* in order to determine *basic variables* and any *free variables* which tells us about the *geometry of the solution space*.
5. Describe the *solution space* using *parametric vector form*.

:::

:::

::::

## Reminders and Today's Goal

Today, we'll be focused on the vector equation $A\vec{x} = \vec{b}$.

. . . 

Because we've been able to argue equivalency between matrix equations, vector equations, and linear systems though, everything we discuss here is applicable to those other contexts as well!

. . . 

**Goal for Today:** Develop a reusable strategy which will make the work we do in solving $A\vec{x} = \vec{b_1}$ subsequently beneficial if we wish to also solve $A\vec{x} = \vec{b_2},~A\vec{x} = \vec{b_3},~\cdots$

## Motivation for Inverses

+ We've been focused for some time now on solving the matrix equation $A\vec{x} = \vec{b}$ (and its equivalent variations). 
+ None of the approaches we've utilized so far are as simple as the approach you used to solve equations like $2x = 4$ -- just *divide both sides by $2$*. 

    + There's a reason for this, and it was highlighted explicitly during our initial discussion on matrices and arithmetic. 
    + Division by a matrix is not a well-defined operation and we can't simply "divide both sides of an equation by a coefficient matrix". 
    + Indeed, you saw in Warm-Up Problem \#2 today that $AB = AC$ does not necessarily mean $B = C$. 

## Motivation for Inverses (Cont'd)

Matrices have some properties that are not familiar to us:

+ Matrix multiplication is *not commutative*. That is, in general, $AB \neq BA$.
+ Matrix multiplication does not have a cancellation rule. That is, in general, $AB = AC$ does not mean that $B = C$.
+ There is no "matrix division" operation.

## Motivation for Inverses (Cont'd)

Consider the following four equations.

. . .

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

+ You've seen and solved equations like these hundreds of times. 
+ When you solved them (those that are solvable, anyway), you likely made use of a *division* operation. 
+ Let's revisit these equations and their solutions, but since we don't have a *division* operation with matrices, let's try to solve them without using *division*!

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

. . .

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

+ The left-hand side of the equation contains the expression "*$5$ times $x$"*
+ We'd like to manipulate the equation so that the left-hand side contains only the expression "*$x$*".
+ Perhaps we can settle for "*$1x$*", since $1$ is the *multiplicative identity* -- that is, $1x = x$ for all possible values of $x$.
+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?

:::

:::{.column width=35%}

\begin{align} 5x &= 35
\end{align}

:::

::::

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

:::{.nonincremental}

+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?
+ Let's multiply both sides of our equation by $\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain

:::

:::

:::{.column width=35%}

\begin{align} 5x &= 35
\end{align}

:::

::::

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

:::{.nonincremental}

+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?
+ Let's multiply both sides of our equation by $\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain

:::

:::

:::{.column width=35%}

\begin{align} 5x &= 35\\
\implies \frac{1}{5}\left(5x\right) &= \implies \frac{1}{5}\left(35\right)
\end{align}

:::

::::

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

:::{.nonincremental}

+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?
+ Let's multiply both sides of our equation by $\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain

:::

:::

:::{.column width=35%}

\begin{align} 5x &= 35\\
\implies \frac{1}{5}\left(5x\right) &= \implies \frac{1}{5}\left(35\right)\\
\implies \left(\frac{1}{5}\cdot 5\right)x &= 7
\end{align}

:::

::::

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

:::{.nonincremental}

+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?
+ Let's multiply both sides of our equation by $\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain

:::

:::

:::{.column width=35%}

\begin{align} 5x &= 35\\
\implies \frac{1}{5}\left(5x\right) &= \implies \frac{1}{5}\left(35\right)\\
\implies \left(\frac{1}{5}\cdot 5\right)x &= 7\\
\implies 1x &= 7
\end{align}

:::

::::

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

:::{.nonincremental}

+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?
+ Let's multiply both sides of our equation by $\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain

:::

:::

:::{.column width=35%}

\begin{align} 5x &= 35\\
\implies \frac{1}{5}\left(5x\right) &= \implies \frac{1}{5}\left(35\right)\\
\implies \left(\frac{1}{5}\cdot 5\right)x &= 7\\
\implies 1x &= 7\\
\implies x &= 7
\end{align}

:::

::::

. . .

***Note*:** The scalar $\frac{1}{5}$ is the *multiplicative inverse* of the scalar $5$ since $\frac{1}{5}\cdot 5 = 5\cdot \frac{1}{5} = 1$, where $1$ is the *multiplicative identity*.

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Example (i):** $5x = 35$

::::{.columns}

:::{.column width=65%}

:::{.nonincremental}

+ We'll need to somehow change that $5$ into a $1$, but we can't use *division* -- we'll be stuck with multiplication instead. Is there a scalar that we can multiply by and achieve our objective?
+ Let's multiply both sides of our equation by $\frac{1}{5}$, the *multiplicative inverse* of $5$. Doing this, we obtain

:::

:::

:::{.column width=35%}

\begin{align} 5x &= 35\\
\implies \frac{1}{5}\left(5x\right) &= \implies \frac{1}{5}\left(35\right)\\
\implies \left(\frac{1}{5}\cdot 5\right)x &= 7\\
\implies 1x &= 7\\
\implies x &= 7
\end{align}

:::

::::

**Summary:** We can solve the *scalar equation* $ax = b$ by multiplying both sides of the equation by the *multiplicative inverse* of $a$, provided that it exists!

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

. . .

**Equation (ii):** $2x = 0$

\begin{align} 2x&= 0
\end{align}

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Equation (ii):** $2x = 0$

\begin{align} 2x&= 0\\
\implies \frac{1}{2}\left(2x\right) &= \frac{1}{2}\left(0\right)
\end{align}

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Equation (ii):** $2x = 0$

\begin{align} 2x&= 0\\
\implies \frac{1}{2}\left(2x\right) &= \frac{1}{2}\left(0\right)\\
\implies \left(\frac{1}{2}\cdot 2\right)x &= 0
\end{align}

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Equation (ii):** $2x = 0$

\begin{align} 2x&= 0\\
\implies \frac{1}{2}\left(2x\right) &= \frac{1}{2}\left(0\right)\\
\implies \left(\frac{1}{2}\cdot 2\right)x &= 0\\
\implies 1x &= 0
\end{align}

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

**Equation (ii):** $2x = 0$

\begin{align} 2x&= 0\\
\implies \frac{1}{2}\left(2x\right) &= \frac{1}{2}\left(0\right)\\
\implies \left(\frac{1}{2}\cdot 2\right)x &= 0\\
\implies 1x &= 0\\
\implies x &= 0
\end{align}

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

. . .

**Equation (iii):** $0x = 2$...

+ We run into a little bit of a problem here. 
+ There is no multiplicative inverse of $0$. 
+ However, we do know that the product $0x$ for any value of $x$ will be $0$. 
+ This equation has no solutions

    + It is *inconsistent*.

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

. . .

**Equation (iv):** $0x = 0$...

+ Again, we run into a problem. 
+ There is no multiplicative inverse of $0$, so we cannot find a solution. 
+ Inspection however, shows us that any value of $x$ will satisfy this equation.

    + There are infinitely many solutions!

## Motivation for Inverses (Cont'd)

Consider the following four equations.

$$(i)~~5x = 35~~~~~~~~(ii)~~2x = 0~~~~~~~~(iii)~~0x = 2~~~~~~~~(iv)~~0x = 0$$

. . .

**Summary:** The scalar equation $ax = b$ has a *unique solution* if the multiplicative inverse of $a$ (that is, $\frac{1}{a}$) exists. Otherwise, the equation either has no solutions, or infinitely many solutions.

## Invertible Matrices

. . .

Recall that *linear transformations* (a special class of function) correspond to matrix multiplication. 

. . .

You might remember the following two properties that some functions satisfy.

+ **One-to-One:** A function $f$ is said to be *one-to-one* if $f(x_1) = f(x_2)$ implies that $x_1 = x_2$. That is, for each output there is exactly one input being mapped to it.

+ **Onto:** A function $f$ is said to be *onto* if for every $b$ in the codomain of $f$, there is some $x$ in the domain such that $f\left(x\right) = b$.

. . .

These properties are important because if a function is <u>both</u> *one-to-one* and *onto*, then that function is ***invertible***.

## Invertible Matrices

. . .

We can argue that a linear transformation $T\left(\vec{x}\right) = A\vec{x}$ is 

+ *one-to-one* if the matrix $A$ has a pivot in every column. 

    + No free variables $\implies$ unique solutions.
+ *onto* if the matrix $A$ has a pivot in every row .

    + It is impossible then for $\begin{bmatrix} A & | & \vec{b}\end{bmatrix}$ to have a pivot in its rightmost column, so $A\vec{x} = \vec{b}$ is consistent for all choices of $\vec{b}$.

. . .

**Summary/Theorem:** The above properties and discussion argue that the *linear transformation* $T\left(\vec{x}\right) = A\vec{x}$ is *invertible* if and only if the matrix $A$ has a pivot in every row and every column. This means that matrix multiplication via the matrix $A$ is invertible in these scenarios.

## Matrix Inverses

. . .

**Note:** Only square matrices can have a pivot in every row *and* every column. Thus only square matrices are invertible, but not every square matrix is invertible.

. . .

**Definition (Matrix Inverse):** Given an $n\times n$ matrix $A$, a matrix $B$ such that $AB = BA = I_n$ is said to be the inverse of $A$. In this case, we write $B = A^{-1}$.

. . .

**Definition (Singular and Non-Singular Matrices):** A matrix whose inverse exists is said to be a *non-singular* matrix, whereas a matrix whose inverse does not exist is said to to be *singular*.

## Inverse of a $2\times 2$ Matrix (Shortcut)

. . .

Note that the following is a *shortcut* for computing the inverse of a $2\times 2$ matrix, but will not work for larger matrices.

. . .

**Computing the Inverse of a $2\times 2$ Matrix:** Let $A = \left[\begin{array}{rr} a & b\\ c & d\end{array}\right]$ be a $2\times 2$ matrix and assume $ad - bc \neq 0$, then $A$ is invertible and $A^{-1} = \frac{1}{ad - bc}\left[\begin{array}{rr} d & -b\\ -c & a\end{array}\right]$.

. . . 

**Note:** If $A = \left[\begin{array}{rr} a & b\\ c & d\end{array}\right]$ and $ad - bc = 0$, then $A$ is *singular* and $A^{-1}$ does not exist.

. . .

**Preview:** The quantity $ad - bc$ is called the *determinant* of the $2\times 2$ matrix $A = \left[\begin{array}{rr} a &b\\ c &d\end{array}\right]$. We write this as $\det\left(A\right) = ad - bc$. We'll discuss determinants in greater detail later in our course.

## Examples to Try \#1

**Example:** Compute the inverses of the following $2\times 2$ matrices if they exist. If they do not exist, discuss why.

:::{.nonincremental}

+ $A = \begin{bmatrix} 1 & 3\\ -4 & 5\end{bmatrix}$
+ $B = \begin{bmatrix} 1 & -5\\ -2 & 10\end{bmatrix}$ 
+ $C = \begin{bmatrix} 2 & 1\\ -3 & 2\end{bmatrix}$

:::

## Inverse Matrices: Properties and Usage

. . .

**Theorem (Properties of Invertible Matrices):** The following properties about matrices and their inverses are true.

+ If $A$ is an invertible matrix, then $A^{-1}$ is also invertible and $\left(A^{-1}\right)^{-1} = A$.
+ If $A$ and $B$ are invertible $n\times n$ matrices, then $AB$ is also an invertible $n\times n$ matrix. The inverse of $AB$ is $\left(AB\right)^{-1} = B^{-1}A^{-1}$.
+ If $A$ is an invertible matrix then so is $A^T$, and the inverse of $A^T$ is $\left(A^T\right)^{-1} = \left(A^{-1}\right)^T$.

. . .

The property in the first bullet point above allows us a new straegy for solving matrix equations $A\vec{x} = \vec{b}$.

. . .

**Strategy (Invertible Matrices and Solving $A\vec{x} = \vec{b}$):** If the matrix $A$ is invertible, then the matrix equation $A\vec{x} = \vec{b}$ has a unique solution for all $\vec{b}$. That unique solution is $\vec{x} = A^{-1}\vec{b}$.

## Examples to Try \#2

**Example:** Consider the matrix $A = \left[\begin{array}{rr} 5 & -4\\ 1 & 2\end{array}\right]$ and the vectors $\vec{b_1} = \left[\begin{array}{r} -2\\ 7\end{array}\right]$, $\vec{b_2} = \left[\begin{array}{r} -15\\ -9\end{array}\right]$, $\vec{b_3} = \left[\begin{array}{r} -6\\ 10\end{array}\right]$. Find $A^{-1}$ and use it to solve each of the matrix equations $A\vec{x} = \vec{b_i}$ for $i = 1, 2, 3$.

## Constructing Matrix Inverses (General Strategy)

. . .

**Strategy (Constructing the Inverse of a Matrix):** The theorem above gives us a strategy for constructing the inverse of a matrix as long as the inverse exists.

1. Start with an augmented matrix of the form $\left[A\mid I_n\right]$ -- this augmented matrix has the full identity matrix on the right side of that dividing line rather than just a vector.
2. Use our old row-reducing techniques to transform the left hand side of the augmented matrix into the identity matrix. The resuting matrix on the right is $A^{-1}$.

. . .

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

. . .

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] 
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\substack{R_3\leftarrow R_3 + (-2R_1)\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 0 & -5 & 12 & -2 & 0 & 1\end{array}\right]
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\substack{R_3\leftarrow R_3 + (-2R_1)\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 0 & -5 & 12 & -2 & 0 & 1\end{array}\right]\\
&\substack{R_3\leftarrow R_3 + R_2\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 0 & 0 & 16 & -2 & 1 & 1\end{array}\right]
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\substack{R_3\leftarrow R_3 + (-2R_1)\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 0 & -5 & 12 & -2 & 0 & 1\end{array}\right]\\
&\substack{R_3\leftarrow R_3 + R_2\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 0 & 0 & 16 & -2 & 1 & 1\end{array}\right]\\
&\substack{R_2\leftarrow (1/5)R_2\\ R_3\leftarrow (1/16)R_3\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 1 & 4/5 & 0 & 1/5 & 0\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\sim \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 1 & 4/5 & 0 & 1/5 & 0\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\sim \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 1 & 4/5 & 0 & 1/5 & 0\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
&\substack{R_2\leftarrow R_2 + (-4/5R_3)\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\sim \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 1 & 4/5 & 0 & 1/5 & 0\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
&\substack{R_2\leftarrow R_2 + (-4/5R_3)\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
&\substack{R_1\leftarrow R_1 + 2R_3\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 3 & 0 & 3/4 & 1/8 & 1/8\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\sim \left[\begin{array}{rrr|rrr} 1 & 3 & 0 & 3/4 & 1/8 & 1/8\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
\end{align*}

## Completed Example \#2

**Example:** Construct the inverse of the matrix $A = \left[\begin{array}{rrr} 1 & 3 & -2\\ 0 & 5 & 4\\ 2 & 1 & 8\end{array}\right]$ if it exists.

\begin{align*} \left[\begin{array}{rrr|rrr} 1 & 3 & -2 & 1 & 0 & 0\\ 0 & 5 & 4 & 0 & 1 & 0\\ 2 & 1 & 8 & 0 & 0 & 1\end{array}\right] &\sim \left[\begin{array}{rrr|rrr} 1 & 3 & 0 & 3/4 & 1/8 & 1/8\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
&\substack{R_1\leftarrow R_1 + (-3R_2)\\ \longrightarrow} \left[\begin{array}{rrr|rrr} 1 & 0 & 0 & 9/20 & -13/40 & 11/40\\ 0 & 1 & 0 & 1/10 & 3/20 & -1/20\\ 0 & 0 & 1 & -1/8 & 1/16 & 1/16\end{array}\right]\\
\end{align*}

. . .

So $\boxed{~\displaystyle{A^{-1} = \left[\begin{array}{rrr} 9/20 & -13/40 & 11/40\\ 1/10 & 3/20 & -1/20\\ -1/8 & 1/16 & 1/16\end{array}\right]}~}$. $_\blacktriangledown$

## Examples to Try

**Example 1:** Find the inverse of the matrix $\left[\begin{array}{rrr} 1 & 0 & 1\\ -2 & 2 & 4\\ 0 & 6 & -4\end{array}\right]$.

**Example 2:** Solve the system $\left\{\begin{array}{rlr} 2x_1 + 3x_2 & = & 8\\ -x_1 + 5x_2 & = & 9\end{array}\right.$ by constructing the corresponding matrix equation and left-multiplying both sides by the inverse of the coefficient matrix.

**Example 3:** Consider the matrix $A = \left[\begin{array}{rr} 5 & -4\\ 1 & 2\end{array}\right]$ and the vectors $\vec{b_1} = \left[\begin{array}{r} -2\\ 7\end{array}\right]$, $\vec{b_2} = \left[\begin{array}{r} -15\\ -9\end{array}\right]$, $\vec{b_3} = \left[\begin{array}{r} -6\\ 10\end{array}\right]$. 

You solved $A\vec{x} = \vec{b_i}$ earlier by finding and using $A^{-1}$. You could have also solved using our old methods by row reducing the augmented matrix $\left[\begin{array}{c|ccc} A & \vec{b_1} & \vec{b_2} & \vec{b_3}\end{array}\right]$. Discuss any advantages/disadvantages that you see.



## Summary


## Homework

<br/>
<br/>
<br/>

. . .

<center>

$$\Huge{\text{Complete Homework 7}}$$ 
$$\Huge{\text{on MyOpenMath}}$$

</center>

## Next Time...

<br/>
<br/>
<br/>

. . .

<center>

$\Huge{\text{Bases and}}$

$\Huge{\text{Coordinate Systems}}$ 


</center>


